{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094868a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed8cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# genes-pathways annotation\n",
    "\n",
    "path = './kegg_pathway/kegg_hsa.gmt'\n",
    "\n",
    "files = open(path,encoding='utf-8')\n",
    "\n",
    "files = files.readlines()\n",
    "\n",
    "paways_genes_dict = {}\n",
    "for i in files:  \n",
    "    paways_genes_dict[i.split('\\t')[0].split('_')[0]] = i.replace('\\n','').split('\\t')[2:] \n",
    "\n",
    "print(paways_genes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376544fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mirna-pathways annotation\n",
    "path = './kegg_pathway/kegg_anano.txt'\n",
    "\n",
    "files = open(path,encoding='utf-8')\n",
    "\n",
    "files = files.readlines()\n",
    "\n",
    "paways_mirna_dict = {}\n",
    "for i in files:\n",
    "     keys = i.split(',')[0].split('|')[1]\n",
    "     values1 = i.split(',')[1:-1]\n",
    "     values2 =  i.split(',')[-1].replace('\\n','')\n",
    "     values1.append(values2)\n",
    "     values1 =list(set(values1)) \n",
    "     paways_mirna_dict[keys] = values1\n",
    "\n",
    "print(paways_mirna_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27293b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the intersection of paways_genes_dict and paways_mirna_dict passes\n",
    "union_kegg = list(set(paways_genes_dict.keys()).intersection(set(paways_mirna_dict.keys())))\n",
    "print(union_kegg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "paways_genes_dicts ={}\n",
    "paways_mirna_dicts ={}\n",
    "# Obtain the common pathway information of genes and miRNAs\n",
    "for i in union_kegg:\n",
    "    paways_genes_dicts[i] = paways_genes_dict[i]\n",
    "    \n",
    "for i in union_kegg:\n",
    "    paways_mirna_dicts[i] = paways_mirna_dict[i]    \n",
    "print(paways_genes_dicts)\n",
    "print(len(paways_genes_dicts))\n",
    "print(len(paways_mirna_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30fe7bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "genes_existed_pathway = []\n",
    "mirna_existed_pathway = []\n",
    "\n",
    "# Merge all the genes in the public pathways in the dictionary paways_genes_dicts\n",
    "# and store the final result in a de-weighted set genes_existed_pathway.\n",
    "for index in paways_genes_dicts.keys():\n",
    "    genes_existed_pathway = genes_existed_pathway+ list(paways_genes_dicts[index])\n",
    "genes_existed_pathway = set(genes_existed_pathway)\n",
    "\n",
    "\n",
    "for index in paways_mirna_dicts.keys():\n",
    "    mirna_existed_pathway = mirna_existed_pathway+ list(paways_mirna_dicts[index])\n",
    "mirna_existed_pathway = set(mirna_existed_pathway)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8c874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(genes_existed_pathway)\n",
    "print(len(genes_existed_pathway))\n",
    "print(len(mirna_existed_pathway))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3877bbba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loading data\n",
    "snv_data = pd.read_csv(\"./lihc_data/snv_data.csv\",index_col = 0)\n",
    "\n",
    "miRNA_data = pd.read_csv(\"./lihc_data/miRNA_data.csv\",index_col = 0)\n",
    "\n",
    "mRNA_data = pd.read_csv(\"./lihc_data/mRNA_data.csv\",index_col = 0)\n",
    "\n",
    "example_case = pd.read_csv('./lihc_data/response.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8a004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(snv_data.shape)\n",
    "print(mRNA_data.shape)\n",
    "print(miRNA_data.shape)\n",
    "print(example_case.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5295fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Put the Omics data into lists separately\n",
    "union_gene_snv = list(snv_data.columns)\n",
    "union_gene_miRNA = list(miRNA_data.columns)\n",
    "union_gene_mRNA = list(mRNA_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b65eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathway_union = list(paways_genes_dicts.keys())\n",
    "len(pathway_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0608bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine union_gene_snv and union_gene_mRNA into a two-dimensional array\n",
    "mask_list = [union_gene_snv,union_gene_mRNA]\n",
    "# Stored are correspondences between pathways and snv, mRNAs and miRNAs.\n",
    "gene_pathway_bp_dfs = []  \n",
    "\n",
    "\n",
    "for i in range(len(mask_list)):\n",
    "    # Creating a 238 * 1000 matrix of 0 values\n",
    "    pathways_genes = np.zeros((len(pathway_union), len(mask_list[i]))) \n",
    "    for p  in pathway_union:\n",
    "        # Get a list of genes in the pathway\n",
    "        gs = paways_genes_dicts[p]\n",
    "        # Find the gene index in gs that exists in mask_list[i]\n",
    "        g_inds = [mask_list[i].index(g) for g in gs if g in mask_list[i]]\n",
    "        # Get the index of the current route in pathway_union\n",
    "        p_ind = pathway_union.index(p)\n",
    "        # Setting the elements of the matrix corresponding to pathways and genes to 1 indicates the association between them.\n",
    "        pathways_genes[p_ind, g_inds] = 1\n",
    "    # Use the pathways_genes matrix to create a DataFrame whose indexes are pathway names and columns are lists of genes.\n",
    "    gene_pathway_bp = pd.DataFrame(pathways_genes, index=pathway_union, columns=mask_list[i])\n",
    "    \n",
    "   # gene_pathway_bp = gene_pathway_bp.loc[:, (gene_pathway_bp != 0).any(axis=0)]\n",
    "    gene_pathway_bp_dfs.append(gene_pathway_bp)\n",
    "    \n",
    "# The same method gets a DataFrame whose index is the pathway name and columns are the miRNA list.\n",
    "pathways_genes = np.zeros((len(pathway_union), len(union_gene_miRNA))) \n",
    "for p  in pathway_union:\n",
    "    gs = paways_mirna_dicts[p]\n",
    "    g_inds = [union_gene_miRNA.index(g) for g in gs if g in union_gene_miRNA]\n",
    "    p_ind = pathway_union.index(p)\n",
    "    pathways_genes[p_ind, g_inds] = 1\n",
    "gene_pathway_bp = pd.DataFrame(pathways_genes, index=pathway_union, columns=union_gene_miRNA)\n",
    "\n",
    "\n",
    "# gene_pathway_bp = gene_pathway_bp.loc[:, (gene_pathway_bp != 0).any(axis=0)]\n",
    "gene_pathway_bp_dfs.append(gene_pathway_bp)\n",
    "    \n",
    "# gene_pathway_bp_dfs.to_cvs(\"gene_pathway_bp_dfs.csv\")\n",
    "combined_df = pd.concat(gene_pathway_bp_dfs, ignore_index=True)\n",
    "\n",
    "# Write the merged DataFrame to a CSV file\n",
    "#combined_df.to_csv(\"gene_pathway_bp_dfs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "125e5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import glorot_uniform, Initializer\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, GlobalAveragePooling1D,Layer\n",
    "from tensorflow.keras import initializers,activations,regularizers\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11f21ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Biological Pathway Module\n",
    "# A matrix of relationships between mRNAs, miRNAs, SNVs and their respective pathways was constructed with multi-omics data\n",
    "class Biological_module(Layer):\n",
    "    #initialization Parameter \n",
    "    def __init__(self, units, mapp=None, nonzero_ind=None, kernel_initializer='glorot_uniform', W_regularizer=None,\n",
    "                 activation='tanh', use_bias=True,bias_initializer='zeros', bias_regularizer=None,\n",
    "                 bias_constraint=None,**kwargs):\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.mapp = mapp\n",
    "        self.nonzero_ind = nonzero_ind\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(W_regularizer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activation_fn = activations.get(activation)\n",
    "        super(Biological_module, self).__init__(**kwargs)\n",
    "\n",
    "     #build module\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[1]\n",
    "        if not self.mapp is None:\n",
    "            self.mapp = self.mapp.astype(np.float32)\n",
    "        if self.nonzero_ind is None:\n",
    "            nonzero_ind = np.array(np.nonzero(self.mapp)).T\n",
    "            self.nonzero_ind = nonzero_ind\n",
    "        self.kernel_shape = (input_dim, self.units) \n",
    "        nonzero_count = self.nonzero_ind.shape[0]  \n",
    "         # node and node  connection nunber \n",
    "        self.kernel_vector = self.add_weight(name='kernel_vector',\n",
    "                                             shape=(nonzero_count,),\n",
    "                                             initializer=self.kernel_initializer,\n",
    "                                             regularizer=self.kernel_regularizer,\n",
    "                                             trainable=True)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer\n",
    "                                        )\n",
    "        else:\n",
    "            self.bias = None\n",
    "        super(Biological_module, self).build(input_shape)  \n",
    "     #Define a customized neural network layer with sparse connections, optional bias terms and activation functions\n",
    "    def call(self, inputs):\n",
    "              \n",
    "        trans = tf.scatter_nd(tf.constant(self.nonzero_ind, tf.int32), self.kernel_vector,\n",
    "                           tf.constant(list(self.kernel_shape)))\n",
    "        output = K.dot(inputs, trans)\n",
    "    \n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias)\n",
    "            \n",
    "        if self.activation_fn is not None:\n",
    "            output = self.activation_fn(output)\n",
    "\n",
    "        return output\n",
    "    # get module config\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "            'activation': self.activation,\n",
    "            'use_bias': self.use_bias,\n",
    "            'nonzero_ind': np.array(self.nonzero_ind),\n",
    "          \n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'W_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "\n",
    "        }\n",
    "        base_config = super(Biological_module, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "      \n",
    "        return (input_shape[0], self.units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bab00da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple Attention Module,Learning inter-sample correlations through multiple attention mechanisms\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, output_dim, num_heads,dropout_rate=0.1, W_regularizer=None, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.kernel_regularizer = regularizers.get(W_regularizer)\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert self.output_dim % self.num_heads == 0, \"Output dimension must be divisible by the number of heads.\"\n",
    "        self.depth = self.output_dim // self.num_heads\n",
    "\n",
    "        self.query_kernel = self.add_weight(name='query_kernel',\n",
    "                                            shape=(input_shape[-1], self.output_dim),\n",
    "                                            initializer='uniform',\n",
    "                                            regularizer=self.kernel_regularizer,\n",
    "                                            trainable=True)\n",
    "        self.key_kernel = self.add_weight(name='key_kernel',\n",
    "                                          shape=(input_shape[-1], self.output_dim),\n",
    "                                          initializer='uniform',\n",
    "                                          regularizer=self.kernel_regularizer,\n",
    "                                          trainable=True)\n",
    "        self.value_kernel = self.add_weight(name='value_kernel',\n",
    "                                            shape=(input_shape[-1], self.output_dim),\n",
    "                                            initializer='uniform',\n",
    "                                            regularizer=self.kernel_regularizer,\n",
    "                                            trainable=True)\n",
    "        self.dropout = Dropout(self.dropout_rate)\n",
    "        super(MultiHeadAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x,training=False):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "\n",
    "        # Linear projections\n",
    "        Q = tf.tensordot(x, self.query_kernel, axes=[-1, 0])  # (batch_size, seq_len, output_dim)\n",
    "        K = tf.tensordot(x, self.key_kernel, axes=[-1, 0])    # (batch_size, seq_len, output_dim)\n",
    "        V = tf.tensordot(x, self.value_kernel, axes=[-1, 0])  # (batch_size, seq_len, output_dim)\n",
    "\n",
    "        # Reshape to (batch_size, seq_len, num_heads, depth)\n",
    "        Q = tf.reshape(Q, (batch_size, -1, self.num_heads, self.depth))\n",
    "        K = tf.reshape(K, (batch_size, -1, self.num_heads, self.depth))\n",
    "        V = tf.reshape(V, (batch_size, -1, self.num_heads, self.depth))\n",
    "\n",
    "        # Transpose to (batch_size, num_heads, seq_len, depth)\n",
    "        Q = tf.transpose(Q, perm=[0, 2, 1, 3])\n",
    "        K = tf.transpose(K, perm=[0, 2, 1, 3])\n",
    "        V = tf.transpose(V, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "        QK = QK / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "        QK = tf.nn.softmax(QK, axis=-1)\n",
    "        QK = self.dropout(QK, training=training)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        attention_output = tf.matmul(QK, V)  # (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "        # Transpose and reshape back to (batch_size, seq_len, output_dim)\n",
    "        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
    "        attention_output = tf.reshape(attention_output, (batch_size, -1, self.output_dim))\n",
    "        attention_output = self.dropout(attention_output, training=training)\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "        }\n",
    "        base_config = super(MultiHeadAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], self.output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "020e30a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a model\n",
    "def create_model(snv_data,mRNA_data,miRNA_data): \n",
    "\n",
    "    #Input Layer Definition\n",
    "    S_inputs_snv = Input(shape=(snv_data.shape[1],), dtype='float32',name= 'snv_inputs')\n",
    "    S_inputs_mRNA = Input(shape=(mRNA_data.shape[1],), dtype='float32',name= 'mRNA_inputs')\n",
    "    S_inputs_miRNA = Input(shape=(miRNA_data.shape[1],), dtype='float32',name= 'miRNA_inputs')\n",
    "    \n",
    "    #Calling the Biological Pathway Module\n",
    "    h0_snv = Biological_module(gene_pathway_bp_dfs[0].shape[0],mapp =gene_pathway_bp_dfs[0].values.T, name = 'h0_snv',W_regularizer=l2(0.001))(S_inputs_snv)\n",
    "    h0_mRNA = Biological_module(gene_pathway_bp_dfs[1].shape[0],mapp =gene_pathway_bp_dfs[1].values.T, name = 'h0_mRNA',W_regularizer=l2(0.001))(S_inputs_mRNA)\n",
    "    h0_miRNA = Biological_module(gene_pathway_bp_dfs[2].shape[0],mapp =gene_pathway_bp_dfs[2].values.T, name = 'h0_miRNA',W_regularizer=l2(0.001))(S_inputs_miRNA)\n",
    "\n",
    "    #Multiple Self-Attention Mechanism\n",
    "    atten1 = MultiHeadAttention(output_dim=128, num_heads=16, W_regularizer=l2(0.001))(h0_snv)\n",
    "    atten2 = MultiHeadAttention(output_dim=128, num_heads=16, W_regularizer=l2(0.01))(h0_mRNA)\n",
    "    atten3 = MultiHeadAttention(output_dim=128, num_heads=16, W_regularizer=l2(0.01))(h0_miRNA)\n",
    "\n",
    "    #Hierarchical feature fusion\n",
    "    atten12 = tf.keras.layers.concatenate([atten1, atten2])\n",
    "    atten13 = tf.keras.layers.concatenate([atten1, atten3])\n",
    "    feature_tal = tf.keras.layers.concatenate([atten12, atten13])\n",
    "    #full connectivity layer\n",
    "    h2 = tf.keras.layers.Dense(64, activation='tanh')(feature_tal)\n",
    "    h3 = tf.keras.layers.Dense(32, activation='tanh')(h2)\n",
    "    h4 = tf.keras.layers.Dense(8, activation='tanh')(h3)\n",
    "    h5 = tf.keras.layers.Dense(1, activation='sigmoid')(h4)\n",
    "\n",
    "    # Convert the output from a 3D array to a 2D array (model modification)\n",
    "    # Use tf.squeeze to remove dimensions of length 1\n",
    "    h5 = tf.squeeze(h5, axis=-2)\n",
    "\n",
    "    model = Model(inputs=[S_inputs_snv,S_inputs_mRNA,S_inputs_miRNA], outputs=h5)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 0.0001,decay=0.0001) \n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1044dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Evaluation function\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score   \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "def get_metrics(true_score,pre_score,pre_probe):\n",
    "    \n",
    "    # Calculate the True Positive Rate (TPR) and False Positive Rate (FPR) and the corresponding thresholds. pos_label=1 specifies the label of the positive class\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(true_score, pre_probe, pos_label=1)\n",
    "\n",
    "    # Calculate Area Under the Curve (AUC) \n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    # Average Precision Score, AUPR\n",
    "    aupr = average_precision_score(true_score, pre_probe)\n",
    "    \n",
    "    # Precision and Recall\n",
    "    pre, rec, thresholds = precision_recall_curve(true_score, pre_probe) \n",
    "\n",
    "    # AUPRC\n",
    "    auprc  = metrics.auc(rec, pre)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(true_score,pre_score)\n",
    "    \n",
    "    # F1 Score\n",
    "    f1 = metrics.f1_score(true_score, pre_score)\n",
    "    \n",
    "    # Calculation of the precision rate\n",
    "    precision = metrics.precision_score(true_score,pre_score)\n",
    "    # Calculating Recall\n",
    "    recall = metrics.recall_score(true_score,pre_score)\n",
    "\n",
    "    return precision,accuracy,recall,f1,auc,aupr,auprc\n",
    "\n",
    "\n",
    "\n",
    "#Calculation of indicators for each assessment\n",
    "def evaluates(y_test, y_pred):\n",
    "    \n",
    "    auc = metrics.roc_auc_score(y_test,y_pred)\n",
    "    \n",
    "    aupr = average_precision_score(y_test, y_pred)\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred)    \n",
    "    auprc  = metrics.auc(recall, precision)\n",
    "    \n",
    "    pp = [1 if index>=0.5  else 0 for index in  y_pred ]\n",
    "    \n",
    "    pre = metrics.precision_score(y_test,pp)\n",
    "    \n",
    "    f1 = metrics.f1_score(y_test,pp)\n",
    "    \n",
    "    rec = metrics.recall_score(y_test,pp)\n",
    "    \n",
    "    acc = metrics.accuracy_score(y_test,pp)\n",
    "    \n",
    "    print(confusion_matrix(y_test,pp))\n",
    "    return pre,acc,rec,f1,auc,aupr,auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "825824f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "snv_data = snv_data.loc[example_case.index]\n",
    "\n",
    "miRNA_data = miRNA_data.loc[example_case.index]\n",
    "\n",
    "mRNA_data = mRNA_data.loc[example_case.index]\n",
    "\n",
    "example_case = example_case.loc[example_case.index]\n",
    "\n",
    "y = example_case['response'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad9be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_samples =example_case['response'].values\n",
    "\n",
    "print(len(n_samples),n_samples.sum(),(len(n_samples) -n_samples.sum()))\n",
    "\n",
    "x_0 =  len(n_samples) / (2*  (len(n_samples) -n_samples.sum()))\n",
    "# Sample size for each event as a proportion of the total sample size\n",
    "x_1 =  len(n_samples) / (2*  n_samples.sum())\n",
    "\n",
    "print(x_0,x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7830de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  5-fold cross validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# n_splits=5: This parameter specifies the number of folds for cross-validation. In this example, the dataset will be divided into 5 parts.\n",
    "# shuffle=True: This parameter specifies whether the data should be randomly disrupted before splitting.\n",
    "# Setting it to True means that the data will be randomized before the fold is created, which helps to reduce possible biases in the training order.\n",
    "# random_state=1029: This parameter is used to set the seed of the random number generator to ensure that you get the same random permutation every time you run the code.\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=1029) \n",
    "\n",
    "kfscore = []\n",
    "i = 0\n",
    "for train_index, test_index in skf.split(snv_data.values,y):\n",
    "    \n",
    "    snv_train_x = snv_data.values[train_index]\n",
    "    snv_test_x  = snv_data.values[test_index]\n",
    "\n",
    "    mRNA_train_x = mRNA_data.values[train_index]\n",
    "    mRNA_test_x  = mRNA_data.values[test_index]\n",
    "\n",
    "    miRNA_train_x = miRNA_data.values[train_index]\n",
    "    miRNA_test_x  = miRNA_data.values[test_index]\n",
    "\n",
    "    train_y  = y[train_index]\n",
    "    test_y   = y[test_index]\n",
    "\n",
    "    model = create_model(snv_data,mRNA_data,miRNA_data)\n",
    "    model.fit( {\"snv_inputs\": snv_train_x,  \"mRNA_inputs\": mRNA_train_x, 'miRNA_inputs':miRNA_train_x},train_y,\n",
    "                 validation_data=({\"snv_inputs\": snv_test_x, \"mRNA_inputs\": mRNA_test_x,'miRNA_inputs':miRNA_test_x},test_y),\n",
    "                 epochs=200,batch_size = 64,class_weight = {0:x_0,1:x_1})  \n",
    "\n",
    "    y_pred = model.predict({\"snv_inputs\": snv_test_x, \"mRNA_inputs\": mRNA_test_x,'miRNA_inputs':miRNA_test_x})\n",
    "\n",
    "    y_score = [1 if index>=0.5  else 0 for index in  y_pred]\n",
    "\n",
    "    evaluate_epoch = get_metrics(test_y,y_score,y_pred)\n",
    "    print(evaluate_epoch)\n",
    "    kfscore.append(evaluate_epoch)\n",
    "    \n",
    "results = list(np.array(kfscore).sum(axis= 0)/5.0)\n",
    "print('Cross validated results :  ACC = {}, REC = {}, F1 = {}, AUC = {}, AUPR ={}'.format(results[1],results[2],results[3],results[4],results[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53de1a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_index, test_index =  train_test_split(snv_data.values, test_size=0.2, random_state=0)\n",
    "# print(train_index)\n",
    "# print(test_index)\n",
    "X_train_snv, X_test_snv, X_train_mRNA, X_test_mRNA, X_train_miRNA, X_test_miRNA, y_train, y_test = train_test_split(\n",
    "    snv_data.values, mRNA_data.values, miRNA_data.values, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3257702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpretability of the model\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creating Data Sets\n",
    "X_train_snv, X_test_snv, X_train_mRNA, X_test_mRNA, X_train_miRNA, X_test_miRNA, y_train, y_test = train_test_split(\n",
    "    snv_data.values, mRNA_data.values, miRNA_data.values, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Creating Models\n",
    "model = create_model(snv_data,mRNA_data,miRNA_data)\n",
    "#model training\n",
    "model.fit( {\"snv_inputs\": X_train_snv,  \"mRNA_inputs\": X_train_mRNA, 'miRNA_inputs':X_train_miRNA},y_train,\n",
    "                validation_data=({\"snv_inputs\": X_test_snv, \"mRNA_inputs\": X_test_mRNA,'miRNA_inputs':X_test_miRNA},y_test),\n",
    "                epochs=200,batch_size = 64,class_weight = {0:x_0,1:x_1})\n",
    "\n",
    "# Predicting or evaluating in this model\n",
    "y_pred = model.predict({\"snv_inputs\": X_test_snv, \"mRNA_inputs\": X_test_mRNA,'miRNA_inputs':X_test_miRNA})\n",
    "\n",
    "#Training data, splicing training data from three omics\n",
    "X_train = [X_train_snv[:100], X_train_mRNA[:100], X_train_miRNA[:100]]\n",
    "\n",
    "# Creating the SHAP interpreter\n",
    "explainer = shap.DeepExplainer(model, X_train)\n",
    "\n",
    "\n",
    "shap.explainers._deep.deep_tf.op_handlers[\"AddN\"] = shap.explainers._deep.deep_tf.passthrough\n",
    "\n",
    "shap.explainers._deep.deep_tf.op_handlers[\"BatchMatMulV2\"] = shap.explainers._deep.deep_tf.passthrough\n",
    "X_test  = [X_test_snv, X_test_mRNA, X_test_miRNA]\n",
    "\n",
    "\n",
    "# Calculating the SHAP value\n",
    "shap_values = explainer.shap_values(X_test )\n",
    "\n",
    "\n",
    "# Visualize the SHAP value\n",
    "feature_snv = snv_data.columns.to_numpy()  # First input feature name\n",
    "feature_mRNA = mRNA_data.columns.to_numpy()  # Feature name for the second input\n",
    "feature_miRNA = miRNA_data.columns.to_numpy()  # Third input feature name\n",
    "feature_names = [feature_snv, feature_mRNA, feature_miRNA]\n",
    "\n",
    "# summary Plot\n",
    "shap.summary_plot(shap_values[0][0], X_test[0], feature_names=feature_snv, plot_type=\"bar\",max_display=10)\n",
    "shap.summary_plot(shap_values[0][1], X_test[1], feature_names=feature_mRNA, plot_type=\"bar\",max_display=10)\n",
    "shap.summary_plot(shap_values[0][2], X_test[2], feature_names=feature_miRNA, plot_type=\"bar\",max_display=10)\n",
    "\n",
    "for i, shap_values_array in enumerate(shap_values[0]):\n",
    "     shap.summary_plot(shap_values_array, X_test[i], feature_names=feature_names[i], plot_type=\"bar\")\n",
    "\n",
    "# summary Plot\n",
    "shap.summary_plot(shap_values[0][0], X_test[0], feature_names=feature_snv,max_display=10)\n",
    "shap.summary_plot(shap_values[0][1], X_test[1], feature_names=feature_mRNA, max_display=10)\n",
    "shap.summary_plot(shap_values[0][2], X_test[2], feature_names=feature_miRNA, max_display=10)\n",
    "\n",
    "\n",
    "#  Beeswarm \n",
    "shap_values_snv = shap_values[0][0]  # HAP value of snv\n",
    "shap_values_mRNA = shap_values[0][1]  # HAP value of mRNA\n",
    "shap_values_miRNA = shap_values[0][2]  # HAP value of miRNA\n",
    "# snv Beeswarm\n",
    "shap.plots.beeswarm(shap.Explanation(shap_values_snv, feature_names=feature_snv))\n",
    "# mRNA Beeswarm\n",
    "shap.plots.beeswarm(shap.Explanation(shap_values_mRNA, feature_names=feature_mRNA))\n",
    "# miRNA Beeswarm\n",
    "shap.plots.beeswarm(shap.Explanation(shap_values_miRNA, feature_names=feature_miRNA))\n",
    "\n",
    "# Heatmap\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# snv Heatmap\n",
    "shap.plots.heatmap(shap.Explanation(shap_values_snv, feature_names=feature_snv))\n",
    "# mRNA Heatmap\n",
    "shap.plots.heatmap(shap.Explanation(shap_values_mRNA, feature_names=feature_mRNA))\n",
    "# miRNA Heatmap\n",
    "shap.plots.heatmap(shap.Explanation(shap_values_miRNA, feature_names=feature_miRNA))\n",
    "# customize the axis object\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "\n",
    "#If SHAP values are needed for splicing multi-omics features\n",
    "# Splicing SHAP values from different omics\n",
    "shap_values_combined = np.concatenate([shap_values_snv, shap_values_mRNA, shap_values_miRNA], axis=1)\n",
    "# Splice feature name\n",
    "feature_names_combined = np.concatenate([feature_snv, feature_mRNA, feature_miRNA])\n",
    "# Mapping the merged Beeswarm\n",
    "shap.plots.beeswarm(shap.Explanation(shap_values_combined, feature_names=feature_names_combined))\n",
    "# Plotting the merged Heatmap\n",
    "shap.plots.heatmap(shap.Explanation(shap_values_combined, feature_names=feature_names_combined))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepKEGG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
